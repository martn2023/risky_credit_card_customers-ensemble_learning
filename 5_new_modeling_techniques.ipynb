{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82880fcf-da50-4e59-bb0d-88ccdfe1bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b507af4-2dc0-46b5-9465-ff1102b29b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_credit_card_info = imported_credit_card_info.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "imported_credit_card_info.head() #confirming order is now randomized\n",
    "\n",
    "target_column = 'default payment next month'\n",
    "\n",
    "# features, and yes a I know a lot of people use a capital X but it's giving me an OCD attack being inconsitent with a lowercase y\n",
    "x = imported_credit_card_info.drop(columns=[target_column])\n",
    "\n",
    "y = imported_credit_card_info[target_column] # target values, whether it was in default or not\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f25dbf-0741-4773-be00-9d422d880110",
   "metadata": {},
   "source": [
    "# I. 2nd Analysis, But With Boosting\n",
    "\n",
    "### A. Boosting Tools\n",
    "\n",
    "One boosting technique we can use is AdaBoost (name comes from \"Adaptive Boosting\").\n",
    "\n",
    "###  B. Boosting Results\n",
    "**90% recall** on deadbeats who are gonna default? That's wonderful, right? Problem is we only had a 28% precision rate, which means we were crying wolf 72% of the time. Let's trial and error the threshold upwards to see if we get something with a 50% precision, and see how much recall we'd have to give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b048be0-d4ed-4d92-bfa5-541073273a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AdaBoost (threshold=0.3) ===\n",
      "AUC: 0.7664395307408565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.35      0.50      4685\n",
      "           1       0.28      0.90      0.42      1315\n",
      "\n",
      "    accuracy                           0.47      6000\n",
      "   macro avg       0.60      0.62      0.46      6000\n",
      "weighted avg       0.78      0.47      0.49      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=47)\n",
    "ada.fit(x_train, y_train)\n",
    "\n",
    "y_proba = ada.predict_proba(x_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.3).astype(int)  # using your preferred threshold\n",
    "\n",
    "print(\"=== AdaBoost (threshold=0.3) ===\")\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65c693-71af-40e1-9678-81242106eec3",
   "metadata": {},
   "source": [
    "###  C. Boosting Results After We Tone Down the Aggression\n",
    "\n",
    "######  1. Recall at 56%\n",
    "Which is less than our first go at AdaBoost, but still stronger than what we concluded for Random Forest. Let's take a wait-and-see approach i.e. do the stacking first.\n",
    "\n",
    "######  2. Downlplaying F1\n",
    "The F1 metric is defined as (precision*recall)/(precision + recall). The score is punished by extremeties i.e. one of those metrics being really low. This may be important in other scenarios, where there's a balance of priorities, I'm arguing that failing (low recall) to catch a deadbeat before they default is more more expensive than walking away from/irritating a happy customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "852b64f4-d28e-4920-852f-7e0c1aba49fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AdaBoost (threshold=0.385) ===\n",
      "AUC: 0.7664395307408565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      4685\n",
      "           1       0.50      0.56      0.53      1315\n",
      "\n",
      "    accuracy                           0.78      6000\n",
      "   macro avg       0.68      0.70      0.69      6000\n",
      "weighted avg       0.79      0.78      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(random_state=47)\n",
    "ada.fit(x_train, y_train)\n",
    "\n",
    "y_proba = ada.predict_proba(x_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.385).astype(int)  # using a threshold a bit higher/stricter than before, which was 0.30\n",
    "\n",
    "print(\"=== AdaBoost (threshold=0.385) ===\")\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5180d6-c0f1-48f3-b34b-1c824c14c73a",
   "metadata": {},
   "source": [
    "# I. 3nd Analysis, But With Stacking\n",
    "\n",
    "### A. Stacking Tools\n",
    "Now, I'm gonna put on my \"coach\" hat and see how 3 different players work independently of each other, and then with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a89ff835-e52a-4483-a143-5269608e66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7810366390592093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.68      0.78      4685\n",
      "           1       0.39      0.72      0.51      1315\n",
      "\n",
      "    accuracy                           0.69      6000\n",
      "   macro avg       0.64      0.70      0.64      6000\n",
      "weighted avg       0.79      0.69      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf',  RandomForestClassifier(random_state=48, n_estimators=500)),\n",
    "        ('gb',  GradientBoostingClassifier(random_state=48)),\n",
    "        ('ada', AdaBoostClassifier(random_state=48)),\n",
    "    ],\n",
    "    final_estimator=make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(\n",
    "            solver='saga',        # more robust here\n",
    "            max_iter=5000,\n",
    "            class_weight='balanced',\n",
    "            C=0.5                 # a bit more regularization helps\n",
    "        )\n",
    "    ),\n",
    "    passthrough=True,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack.fit(x_train, y_train)\n",
    "y_proba = stack.predict_proba(x_test)[:, 1]\n",
    "y_pred  = (y_proba >= 0.4).astype(int)\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08d9ab-ab46-4dde-b9ee-db97742f42b0",
   "metadata": {},
   "source": [
    "###  B. Stacking Results\n",
    "**90% recall** on deadbeats who are gonna default? That's wonderful, right? Problem is we only had a 28% precision rate, which means we were crying wolf 72% of the time. Let's trial and error the threshold upwards to see if we get something with a 50% precision, and see how much recall we'd have to give up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1005c1a-b3b4-4680-8f4d-69f92e8e1c9a",
   "metadata": {},
   "source": [
    "# III. Actionable Advice\n",
    "### A. Summary of Results From 3 Trials\n",
    "We're seeing different models that can predict defaults from credit card holders with anywhere from 35 to 90% recall, but there's obviously a tradeoff on precision. The Stack method produced a 72% recall, 39% precision mix.\n",
    "\n",
    "### B. How Should We Advise The Bank?\n",
    "##### 1. Value of of Keeping a Good Customer\n",
    "- Average spend about $20,000 a year\n",
    "- Typical lifespan of a developed and competitive market like Taiwan is 6 years, but let's assume this analysis is occuring at the midpoint of 3 years so there's only 3 years remaining. 3x20K USD = 60K USD total spend\n",
    "- Assume ~ 3% in credit card fees, but only 1.6% goes to the bank because some goes to the network (Visa or Mastercard) and some goes to the merchant's bank\n",
    "\n",
    "**1.6% of 120K = 960 USD in swipe revenue, assuming zero interest payments**.\n",
    "\n",
    "##### 2. Value of of Stopping a Bad Customer\n",
    "Allowing a default risk to continue spending for 2 more months = 2 x 2400 = $4,800\n",
    "\n",
    "##### 3. Cost/Benefit Analysis\n",
    "This is admittedly an oversimplification because we don't have internal data on\n",
    "- How many customers would stay with this credit card provider if they received a heaavy-handed message telling them their line is being frozen or their monthly spend was being capped until they returned to better standing\n",
    "- How long the lender would look the other way on default risks\n",
    "- How many of the default risks would come back and eventually pay the bill, and how that would change if they were cut off\n",
    "\n",
    "But in the interest of at least brainstorming a discussion with the C-suite, let's toss this into the meeting: **it may cost 5x as much to keep a bad customer than to cry \"wolf\" andf lose a borderline customer**. \n",
    "\n",
    "So if recall of 70% x 4800 (savings by shutting down a deadbeat) = 3,360\n",
    "And errors on precision (100% -39%) of 61% x 960 = 586\n",
    "\n",
    "**The net \"gain\" from following the stacking strategy is 3360 - 586 = 2,774 USD per customer decision.**\n",
    "\n",
    "##### 4. 2nd and 3rd-Order Effects\n",
    "In the real world, but outside the scope of this applied ML demonstration, the leadership team would need to consider:\n",
    "- If we tighten up on our high-risk customers, will people who would have eventually paid the bill switch to another credit card carrier and given them the next 6 years of revenue + some interest + late fees?\n",
    "- Would both eventual payers and deadbeats screech on social media and to their friends, influencing other borrowers from dealing with our bank?\n",
    "- Shouldn't we have a conversation about why we are evaluating these credit card holders so fervently after the fact, instead of screening them more tightly before we issued the card?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
